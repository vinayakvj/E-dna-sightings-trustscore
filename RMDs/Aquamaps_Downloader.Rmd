---
title: "Aquamaps Downloader and Location Processor"
author: "Nathan Reed (24110024)"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Process AquaMaps Location Data Per Species

## Download Species Location Data

Tool to download information from Aquamaps

```{r}
library(dplyr)
library(fs)
library(terra)
library(ncdf4)
library(curl)
library(tools)

# Import list of species from Philipp's file
tsvcolnames <- c("Species","DNA_Counts")
species_list <- read_tsv("../Data/OceanGenomes.CuratedNT.NBDLTranche1and2and3.CuratedBOLD.species_counts.tsv", col_names=tsvcolnames)
all_species <- species_list$Species
all_species <- unique(all_species)

# Import list of species from our own data
data <- read_csv("../Data/data_GBIF_DNA_enriched.csv")

# Merge with Philipp's list removing "dropped", NAs, trailing white space and duplicates
all_species <- c(all_species, unique(data$species)) %>% trimws %>% unique() %>%   setdiff(c("", NA, "dropped"))

# Create empty lists to hold the data
urls        <- c()
destfiles   <- c()

# Download the AquaMaps data for each species to the "AquaMaps" folder in the "Data" folder
for(i in all_species){
    i        <- gsub(" ", "_", i)
    url      <- paste0("https://thredds.d4science.org/thredds/fileServer/public/netcdf/AquaMaps_11_2019/", i, ".nc")
    destfile <- paste0("../Data/AquaMaps/", i, ".nc")
    destfile <- gsub(':', 'X', destfile )
    if(! file.exists(paste0("../Data/AquaMaps/", i, ".nc")) ) {
        urls      <- c(urls, url)
        destfiles <- c(destfiles, destfile)
    }
}

res       <- multi_download(urls, destfiles)
delete_us <- res$destfile[res$status_code == "404"]
file.remove(delete_us)
```


### Inspect a single Aquamaps NC file to understand the NC format

```{r}
# Choose a single file (change name to one that exists in ../Data/AquaMaps/)
file <- "../Data/AquaMaps/Diaphus_aliciae.nc"

# Load as a raster
r <- rast(file)

# Inspect contents
r

#> class       : SpatRaster 
#> dimensions  : 360, 720, 1  (nrow, ncol, nlyr)
#> resolution  : 0.5, 0.5  (degrees)
#> extent      : -180, 180, -90, 90
#> coord. ref. : lon/lat WGS84 
#> source(s)   : ../Data/AquaMaps/Thunnus_albacares.nc 
#> name(s)     : Probability   (or similar)

# Plot the location data
plot(r, main="Predicted habitat probability")

# Extract the probability for a species at a specific location
sample_lat <- -28.67171667
sample_lon <- 111.4407833
value <- extract(r, cbind(sample_lon, sample_lat))
print(value)

```
### Identify corrupted / unopenable files and remove them

About half the files downloaded aren't valid distribution data (not sure if this is due to a lack of a match to the species name or if AquaMaps simple doesn't have any data). This code block deletes these files.

```{r}
# PART A: AUDIT OF DOWNLOADED .NC FILES TO SEE WHAT'S BROKEN
nc_dir <- "../Data/AquaMaps"
nc_files <- dir_ls(nc_dir, glob = "*.nc")

audit <- tibble(
  file  = nc_files,
  token = basename(nc_files),
  size  = file_info(nc_files)$size
) %>%
  mutate(
    is_zero = size == 0
  )

# Helper: detect file "signature" (NetCDF-3 vs HDF5 vs other)
sig_kind <- function(path) {
  con <- file(path, "rb"); on.exit(close(con), add = TRUE)
  raw <- try(readBin(con, what = "raw", n = 8), silent = TRUE)
  if (inherits(raw, "try-error") || length(raw) == 0) return("unreadable")
  # NetCDF-3 starts with 'CDF'
  if (length(raw) >= 3 && identical(raw[1:3], charToRaw("CDF"))) return("netcdf3")
  # NetCDF-4/HDF5 starts with 0x89 0x48 0x44 0x46 i.e. \x89 H D F
  if (length(raw) >= 4 && identical(raw[1:4], as.raw(c(0x89, 0x48, 0x44, 0x46)))) return("hdf5")
  # Quick HTML sniff (sometimes a 404 page saved as .nc)
  text <- suppressWarnings(rawToChar(raw[raw >= as.raw(0x20) & raw <= as.raw(0x7E)], multiple = TRUE))
  if (length(text) && grepl("DOCTYPE|<html|HTTP", paste(text, collapse = ""), ignore.case = TRUE)) return("html")
  "unknown"
}

audit$signature <- vapply(audit$file, sig_kind, character(1))

# Try opening with ncdf4 as an independent check (GDAL vs NetCDF library)
can_open_ncdf4 <- function(p) {
  out <- try({
    nc <- ncdf4::nc_open(p); on.exit(ncdf4::nc_close(nc), add = TRUE); TRUE
  }, silent = TRUE)
  isTRUE(out)
}
audit$ncdf4_ok <- vapply(audit$file, can_open_ncdf4, logical(1))

# Summary of what's suspicious
audit_summary <- audit %>%
  count(signature, ncdf4_ok, is_zero, name = "n")
print(audit_summary)

# List the worst offenders (not netcdf & can't open)
suspect <- audit %>%
  filter(is_zero | signature %in% c("html", "unknown", "unreadable") | !ncdf4_ok)

# Show which files will be removed
print(suspect)

# Delete the bad files so they can be re-downloaded cleanly
bad_files <- suspect$file
if (length(bad_files)) {
  file.remove(bad_files)
  cat("Deleted", length(bad_files), "corrupted or invalid files.\n")
} else {
  cat("No corrupted files found.\n")
}

```

## Build a function that can loop through the entire dataset and apply a probability to the sample location from the AquaMaps .NC files

```{r}
library(dplyr)

# PART 1: DATA PREPARATION

# Function to convert species name into matching .nc file structure (Sillago bassensis -> Sillago_bassensis.nc)
tok <- function(x) {
  x <- gsub(" ", "_", x)  # AquaMaps convention
  x <- gsub(":", "X", x)  # filesystem-safe
  x
}

# Identify rows with missing/blank/dropped species *before* filename mapping
data <- data %>%
  mutate(
    # use decimalLatitude/decimalLongitude and make numeric helpers for terra::extract
    lat = as.numeric(decimalLatitude),
    lon = as.numeric(decimalLongitude),
    species_trim = trimws(species),
    has_species  = !is.na(species_trim) & species_trim != "" & species_trim != "dropped",
    .token = ifelse(has_species, tok(species_trim), NA_character_),
    .nc    = ifelse(!is.na(.token), file.path("../Data/AquaMaps", paste0(.token, ".nc")), NA_character_),
    am_prob = NA_real_
  )

# Build .nc path per-row, create output column
data <- data %>%
  mutate(
    # keep the same mapping (idempotent); columns are already created above
    .token = .token,
    .nc    = .nc,
    am_prob = am_prob
  )

# Warn about any bad coordinates (still allowed; will return NA)
bad_lat <- which(!is.na(data$lat) & (data$lat < -90 | data$lat > 90))
bad_lon <- which(!is.na(data$lon) & (data$lon < -180 | data$lon > 180))
if (length(bad_lat) || length(bad_lon)) {
  warning(sprintf("Coords out-of-range: lat=%d, lon=%d", length(bad_lat), length(bad_lon)))
}

# PART 2: LOOP THROUGH DATAFILE, PROCESSING LOCATION PROBABILITIES FOR EACH SPECIES AND SAMPLES
species_tokens <- unique(na.omit(data$.token))
missing_rasters <- character()  # to log which species had no .nc file

for (sp in species_tokens) {
  idx <- which(data$.token == sp)
  if (!length(idx)) next

  ncfile <- data$.nc[idx[1]]
  if (!file.exists(ncfile)) {
    message("[skip] Missing raster for species token: ", sp)
    missing_rasters <- c(missing_rasters, sp)
    next
  }

  r <- try(rast(ncfile), silent = TRUE)
  if (inherits(r, "try-error")) {
    warning("[skip] Failed to open raster: ", ncfile)
    next
  }

  # Use only rows with valid numeric coords in bounds
  valid <- which(
    !is.na(data$lon[idx]) & !is.na(data$lat[idx]) &
    data$lon[idx] >= -180 & data$lon[idx] <= 180 &
    data$lat[idx] >=  -90 & data$lat[idx] <=  90
  )
  if (!length(valid)) next  # nothing to extract for this species

  idxv <- idx[valid]
  pts  <- cbind(data$lon[idxv], data$lat[idxv])   # (lon, lat) order!

  # Use Terra library to extract data from the .nc files
  vals_df <- terra::extract(r, pts)               # some versions add an 'ID' column
  if (is.data.frame(vals_df) && "ID" %in% names(vals_df)) {
    vals_df <- vals_df[, setdiff(names(vals_df), "ID"), drop = FALSE]
  }
  vals <- if (is.data.frame(vals_df) && ncol(vals_df) >= 1) vals_df[[1]] else rep(NA_real_, length(idxv))

  # Sanity check: do the lengths of number of species in our data and number of locations from the .nc files match
  if (length(vals) != length(idxv)) {
    stop(sprintf("Sanity failed for '%s': got %d values for %d rows.", sp, length(vals), length(idxv)))
  }

  # Assign back to those valid rows (invalid stay NA)
  data$am_prob[idxv] <- vals
}

# PART 3: REPORT ON THE OVERALL PROCESSING
cat("\nSummary of am_prob:\n")
print(summary(data$am_prob))
cat("Rows with NA am_prob:", sum(is.na(data$am_prob)), "\n")

if (length(missing_rasters)) {
  missing_rasters <- sort(unique(missing_rasters))
  cat("Species tokens with missing .nc files (first 25 shown):\n")
  print(head(missing_rasters, 25))
}

# Optional: which expected files have no match on disk (quick audit)
tokens_expected <- unique(paste0(na.omit(data$.token), ".nc"))
files_on_disk  <- list.files("../Data/AquaMaps", pattern = "\\.nc$", full.names = FALSE)
not_found <- setdiff(tokens_expected, files_on_disk)
cat("Unmatched tokens (no .nc on disk):", length(not_found), "\n")
if (length(not_found)) print(head(not_found, 25))

# Glance at results
head(data[c("species", "decimalLatitude", "decimalLongitude", "am_prob")], 10)

# Export datafile with location probabilities
head(data)
data <- data %>% select(-c(lat, lon, species_trim, .token))

# Number of rows with / without location probability
(sum(!is.na(data$am_prob)))
(sum(is.na(data$am_prob)))

# Number of rows with / without species name
(sum(data$species != "dropped"))
(sum(data$species == "dropped"))
```

## Add Fish Tree of Lift Species Diversification Rates to data

```{r}
# Import tiprates.csv
tiprates <- read_csv("../Data/tiprates.csv")

# Normalise dr (diversification rate) to range 0â€“1
tiprates <- tiprates %>%  mutate(dr_normalised = (dr - min(dr, na.rm = TRUE)) / (max(dr, na.rm = TRUE) - min(dr, na.rm = TRUE)))

# Join dr + dr_normalised into main data by species
data <- data %>% left_join(tiprates %>% select(species, dr, dr_normalised), by = "species")

# Quick check
head(data[c("species", "dr", "dr_normalised")])

# Write out to CSV
write_csv(data, "../Data/data_GBIF_enriched_w_AquaMapsLocProb_DR.csv")
```

